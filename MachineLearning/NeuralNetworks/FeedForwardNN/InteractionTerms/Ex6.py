# -*- coding: utf-8 -*-
"""Exercise_6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W3hnyTcAtKPdWIXPwVILxm0JgMLkUWR0
"""

import random
import pandas as pd
import numpy as np
import seaborn as sns
import math
from datetime import datetime
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.utils import to_categorical
from keras.regularizers import l1,l2
from keras.callbacks import EarlyStopping
from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier
from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, cross_val_score, train_test_split
from sklearn import model_selection
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import f_regression
from sklearn import metrics
from sklearn import preprocessing
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, mean_absolute_error
import statsmodels.api as sm
import statsmodels.stats.diagnostic as tds
from statsmodels.api import add_constant
from scipy import stats

"""**DGP**"""

M = 5000
np.random.seed(7)
X = np.zeros(shape=(M,2))
sigma_n = 0.01
X[:int(M/2),0]= np.random.randn(int(M/2))
X[:int(M/2),1]= np.random.randn(int(M/2))

X[int(M/2):,0]= -X[:int(M/2),0]
X[int(M/2):,1]= -X[:int(M/2),1]

eps= np.random.randn(M)
Y= 1.0*X[:,0] + 1.0*X[:,1] + 1.0*X[:,0]*X[:,1] + sigma_n*eps

"""**Definition of sensitivities function for tanh**"""

# The activation function is tanh
def sensitivities(lm, X):
    
    W1=lm.model.get_weights()[0]
    b1=lm.model.get_weights()[1]
    W2=lm.model.get_weights()[2]
    b2=lm.model.get_weights()[3]
    
    
    M = np.shape(X)[0]
    p = np.shape(X)[1]

    beta=np.array([0]*M*(p+1), dtype='float32').reshape(M,p+1)
    beta_interact=np.array([0]*M*p*p, dtype='float32').reshape(M,p,p)
    
    beta[:,0]= (np.dot(np.transpose(W2),np.tanh(b1)) + b2)[0] # intercept \beta_0= F_{W,b}(0)
    for i in range(M):
 
      Z1 = np.tanh(np.dot(np.transpose(W1),np.transpose(X[i,])) + b1)
      
      D = np.diag(1-Z1**2) 
      D_prime =np.diag(-2*Z1*(1-Z1**2))   # needed for interaction term     
        
      for j in range(p):  
          beta[i,j+1]=np.dot(np.transpose(W2),np.dot(D,W1[j]))
          #interaction term 
          for k in range(p):
            beta_interact[i,j,k]=np.dot(np.transpose(W2),np.dot(np.diag(W1[j]), np.dot(D_prime,W1[k])))  
    
    return(beta, beta_interact)

"""**Fit the model to the data**"""

vector_info = []
for n in [5,10,50,100,500,1000]:
  # n is the number of hidden units
  def linear_NN1_model_act(l1_reg=0.0):    
    model = Sequential()
    model.add(Dense(n, input_dim=2, kernel_initializer='normal', activation='tanh'))
    model.add(Dense(1, kernel_initializer='normal')) 
    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])
    return model
  
  es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=10)
  
  lm = KerasRegressor(build_fn=linear_NN1_model_act, epochs=100, batch_size=10, verbose=1, callbacks=[es])

  lm.fit(X,Y)
  
  beta, beta_inter=sensitivities(lm, X)
  

  int_mean=np.mean(beta_inter, axis=0)
  int_std=np.std(beta_inter, axis=0)
  
  int_meanfin=int_mean[0,1]
  int_stdfin=int_std[0,1]
 
  
  vector_info.append([int_meanfin,int_stdfin])

"""**Results of mean and standard deviation for interaction terms** (Note that even though the distribution of the interaction terms is not Normal, we have plotted the figures using this function to have a better idea of the results. It has been used only to see easly the mean an standard deviation in a graph)"""

# table
import plotly.graph_objects as go

inter = np.array(vector_info)
vector_n = np.array([5,10,50,100,500,1000])

fig = go.Figure(data=[go.Table(header=dict(values=["n hidden units", "mean interaction", "std interaction"]), cells=dict(values=[vector_n, inter[:,0],inter[:,1]]))])

fig.show()


# graphs

import scipy.stats as stats

vector_n = np.array([5,10,50,100,500,1000])

inter_mean = inter[:,0]
inter_std = inter[:,1]

ax = plt.subplot()

for i in range(0,6):
  mu = inter_mean[i]
  sigma = inter_std[i]
  x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)
  ax.plot(x, stats.norm.pdf(x, mu, sigma),label='n='+str(vector_n[i]))
  ax.legend()
  
ax.set_title('interaction terms')

plt.show()