# -*- coding: utf-8 -*-
"""Question5_Sara_Inigo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mbmrexdd95OxP0WbbUSVOqlF1_oJQQZb

**IMPORTANT NOTE: The simulations have been done just with the last 100000 samples, to make simulations easier and faster.**
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
import statsmodels.api as sm
from __future__ import print_function, division
import numpy as np
from keras.layers import Convolution1D, Dense, MaxPooling1D, Flatten
from keras.models import Sequential
from keras.models import load_model
from keras import optimizers
from keras.regularizers import l1

"""Upload of the data"""

df = pd.read_csv('HFT.csv')
df

df.head()

"""We consider a univariate prediction problem where the time series is given by 'feature_3' in the data frame."""

use_features = ['feature_3'] # continuous input
target = 'feature_3' # continuous output

"""**QUESTION 1**. Confirm that the data is stationary by applying the augmented Dickey-Fuller test."""

adf, p, usedlag, nobs, cvs,aic=sm.tsa.stattools.adfuller(df[use_features[0]][:200000].values)
print(adf,p, nobs, cvs)

"""Since -3.97 < -3.43, we reject the null at the 99% confidence level as the p-value is smaller than 0.01. This suggests that the **time series is** **stationary**.

**IMPORTANT NOTE**: We are going to take just the last 100000 samples of the dataset to make simulations easier and faster.
"""

df = df[-100000:]

len(df)

"""**QUESTION 2**. Estimate the partial autocorrelation and determine the optimum lag at the 99% confidence level."""

pacf=sm.tsa.stattools.pacf(df[use_features], nlags=30)

"""We find the first lag which isn't significant at the 99% level and automatically determine the number of lags needed in our autoregressive model as one below this value"""

n_steps=np.where(np.array(np.abs(pacf)>2.58/np.sqrt(len(df[use_features])))==False)[0][0] -1

"""This may lead to a high order model, with more lags than strictly necessary. We could view this value, informally, as an upper bound on the number of lags needed. We can also simply identify the order of the model based on the plot of the PACF. In this case, a minimum of 2 lags appears satisfactory, although more may be needed."""

plt.plot(pacf, label='pacf')
plt.plot([2.58/np.sqrt(len(df[use_features]))]*30, label='99% confidence interval')
plt.legend()

n_steps = 20 # number of lags to include in the model, based on the plot

"""**QUESTION 3 a**. Evaluate the MSE in-sample and out-of-sample using 4 filters.

**Split the data into training and test for scaling the data**
"""

train_weight = 0.8
split = int(len(df)*train_weight)
# train
df_train = df.iloc[:split]

# computation of mu and sigma
mu = np.float(df_train[use_features].mean())
sigma = np.float(df_train[use_features].std())

# standarized train
std_df_train = df_train[use_features].apply(lambda x: (x - mu) / sigma)

# test
df_test = df.iloc[split:]

# standarized test
std_df_test = df[use_features].apply(lambda x: (x - mu) / sigma).iloc[split:]

std_df_train_np = np.transpose((std_df_train[use_features].to_numpy()))
std_df_test_np = np.transpose((std_df_test[use_features].to_numpy()))
std_df_train_np = std_df_train_np[0]
std_df_test_np = std_df_test_np[0]

std_df_train_np

"""**Prediction of high frequency mid-prices with a single hidden layer CNN using 4 filters**

Functions definition
"""

def make_timeseries_regressor(window_size, filter_length, nb_input_series=1, nb_outputs=1, nb_filter=4):

    model = Sequential((
        Convolution1D(nb_filter=nb_filter, filter_length=filter_length, activation='relu', input_shape=(window_size, nb_input_series)),
        Flatten(),
        Dense(nb_outputs, activation='linear', activity_regularizer=l1(0.001)), 
    ))
    model.compile(loss='mse', optimizer='adam', metrics=['mae'])

    return model

def make_timeseries_instances(timeseries, window_size):
    timeseries = np.asarray(timeseries)
    assert 0 < window_size < timeseries.shape[0]
    X = np.atleast_3d(np.array([timeseries[start:start + window_size] for start in range(0, timeseries.shape[0] - window_size)]))
    y = timeseries[window_size:]
    q = np.atleast_3d([timeseries[-window_size:]])
    return X, y, q

np.set_printoptions(threshold=25)
window_size = n_steps
nb_filter = 4
filter_length = 5
timeseries_test = std_df_test_np 
timeseries_train = std_df_train_np

"""Evaluate time series"""

timeseries_train = np.atleast_2d(timeseries_train)
if timeseries_train.shape[0] == 1:
    timeseries_train = timeseries_train.T       # Convert 1D vectors to 2D column vectors

timeseries_test = np.atleast_2d(timeseries_test)
if timeseries_test.shape[0] == 1:
    timeseries_test = timeseries_test.T       # Convert 1D vectors to 2D column vectors

nb_samples, nb_series = timeseries_train.shape
print('\n\nTimeseries ({} samples by {} series):\n'.format(nb_samples, nb_series), timeseries_train)
model = make_timeseries_regressor(window_size=window_size, filter_length=filter_length, nb_input_series=nb_series, nb_outputs=nb_series, nb_filter=nb_filter)
print('\n\nModel with input size {}, output size {}, {} conv filters of length {}'.format(model.input_shape, model.output_shape, nb_filter, filter_length))
model.summary()

X_train, y_train, q_train = make_timeseries_instances(timeseries_train, window_size)
X_test, y_test, q_test = make_timeseries_instances(timeseries_test, window_size)

print('\n\nInput features:', X_train, '\n\nOutput labels:', y_train, '\n\nQuery vector:', q_train, sep='\n')

model.fit(X_train, y_train, nb_epoch=25, batch_size=2, validation_data=(X_test, y_test))

pred = model.predict(X_test)
print('\n\nactual', 'predicted', sep='\t')
for actual, predicted in zip(y_test, pred.squeeze()):
    print(actual.squeeze(), predicted, sep='\t')
print('next', model.predict(q_train).squeeze(), sep='\t')

"""**Model prediction**"""

pred_test = model.predict(X_test)
pred_train = model.predict(X_train)

"""**Model performance**"""

fig = plt.figure(figsize=(16,9))
train_line_real = plt.plot(df_train.index[n_steps:], df_train[use_features][n_steps:], color="g", label="Observed (Training)")
train_line_pred = plt.plot(df_train.index[n_steps:], pred_train[:, 0], color="r", label="CNN Predict (Training)")


plt.legend(loc="best", fontsize=15)
plt.title('Observed vs Model (Training)', fontsize=20)
plt.xlabel('Time', fontsize=20)
plt.ylabel('Y', fontsize=20)


fig = plt.figure(figsize=(16,9))
test_line_real = plt.plot(df_test.index[n_steps:], df_test[use_features][n_steps:], color="g", label="Observed (Testing)")
test_line_pred = plt.plot(df_test.index[n_steps:], pred_test[:, 0], color="r", label="CNN Predict (Testing)")

plt.legend(loc="best", fontsize=15)
plt.title('Observed vs Model (Testing)', fontsize=20)
plt.xlabel('Time', fontsize=20)
plt.ylabel('Y', fontsize=20)

plt.show()

"""**Mean Squared Error (MSE) calculation**"""

MSE_train = mean_squared_error(df_train[use_features][n_steps:], pred_train[:, 0])
print(MSE_train)
MSE_test = mean_squared_error(df_test[use_features][n_steps:], pred_test[:, 0])
print(MSE_test)

"""**QUESTION 3 b**. Implementation for models with more than 4 filters

**5 filters**
"""

# CNN creation
nb_filter = 5
model_5 = make_timeseries_regressor(window_size=window_size, filter_length=filter_length, nb_input_series=nb_series, nb_outputs=nb_series, nb_filter=nb_filter)
print('\n\nModel with input size {}, output size {}, {} conv filters of length {}'.format(model_5.input_shape, model_5.output_shape, nb_filter, filter_length))
model_5.summary()

# fit the model
model_5.fit(X_train, y_train, nb_epoch=25, batch_size=2, validation_data=(X_test, y_test))

# make predictions
pred_test_5 = model_5.predict(X_test)
pred_train_5 = model_5.predict(X_train)

# train MSE and test MSE
MSE_train_5 = mean_squared_error(df_train[use_features][n_steps:], pred_train_5[:, 0])
print(MSE_train_5)
MSE_test_5 = mean_squared_error(df_test[use_features][n_steps:], pred_test_5[:, 0])
print(MSE_test_5)

"""**10 filters**"""

# CNN creation
nb_filter = 10
model_10 = make_timeseries_regressor(window_size=window_size, filter_length=filter_length, nb_input_series=nb_series, nb_outputs=nb_series, nb_filter=nb_filter)
print('\n\nModel with input size {}, output size {}, {} conv filters of length {}'.format(model_10.input_shape, model_10.output_shape, nb_filter, filter_length))
model_10.summary()

# fit the model
model_10.fit(X_train, y_train, nb_epoch=25, batch_size=2, validation_data=(X_test, y_test))

# make predictions
pred_test_10 = model_10.predict(X_test)
pred_train_10 = model_10.predict(X_train)

# train MSE and test MSE
MSE_train_10 = mean_squared_error(df_train[use_features][n_steps:], pred_train_10[:, 0])
print(MSE_train_10)
MSE_test_10 = mean_squared_error(df_test[use_features][n_steps:], pred_test_10[:, 0])
print(MSE_test_10)

"""**20 filters**"""

# CNN creation
nb_filter = 20
model_20 = make_timeseries_regressor(window_size=window_size, filter_length=filter_length, nb_input_series=nb_series, nb_outputs=nb_series, nb_filter=nb_filter)
print('\n\nModel with input size {}, output size {}, {} conv filters of length {}'.format(model_20.input_shape, model_20.output_shape, nb_filter, filter_length))
model_20.summary()

# fit the model
model_20.fit(X_train, y_train, nb_epoch=25, batch_size=2, validation_data=(X_test, y_test))

# make predictions
pred_test_20 = model_20.predict(X_test)
pred_train_20 = model_20.predict(X_train)

# train MSE and test MSE
MSE_train_20 = mean_squared_error(df_train[use_features][n_steps:], pred_train_20[:, 0])
print(MSE_train_20)
MSE_test_20 = mean_squared_error(df_test[use_features][n_steps:], pred_test_20[:, 0])
print(MSE_test_20)

"""**50 filters**"""

# CNN creation
nb_filter = 50
model_50 = make_timeseries_regressor(window_size=window_size, filter_length=filter_length, nb_input_series=nb_series, nb_outputs=nb_series, nb_filter=nb_filter)
print('\n\nModel with input size {}, output size {}, {} conv filters of length {}'.format(model_50.input_shape, model_50.output_shape, nb_filter, filter_length))
model_50.summary()

# fit the model
model_50.fit(X_train, y_train, nb_epoch=25, batch_size=2, validation_data=(X_test, y_test))

# make predictions
pred_test_50 = model_50.predict(X_test)
pred_train_50 = model_50.predict(X_train)

# train MSE and test MSE
MSE_train_50 = mean_squared_error(df_train[use_features][n_steps:], pred_train_50[:, 0])
print(MSE_train_50)
MSE_test_50 = mean_squared_error(df_test[use_features][n_steps:], pred_test_50[:, 0])
print(MSE_test_50)

"""**QUESTION 4**. Apply L1 regularization to reduce the variance.

To apply an L1 regularization it has been added the function "keras.regularizers.l1(0.001)" to the Dense layer of the CNN. We have select a parameter of 0.001

**QUESTION 5**. Determine whether the model error is white noise or is auto-
correlated by applying the Ljung-Box test.
"""

T=100000
residual=df_train[use_features][n_steps:(n_steps+T)].values-(sigma*pred_train[:T, 0]+mu)
lb,p=sm.stats.diagnostic.acorr_ljungbox(residual, lags=10, boxpierce=False)

"""**The Box-Ljung test statistics:**"""

lb

"""**The p-values:**"""

p

df[use_features][20:(20+100000)].values

a=df.iloc[:,0]
a